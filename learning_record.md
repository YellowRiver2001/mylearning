##追踪
1.追踪算法的调研：  
  +传统追踪算法  
  
  》光流法：       

     *光流：空间运动物体在观察成像平面上像素运动的瞬时速度。  
     *光流法：利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。  
     *光流法的前提假设：  
      （1）相邻帧之间的亮度恒定；  
      （2）相邻视频帧的取帧时间连续，或者，相邻帧之间物体的运动比较“微小”；  
      （3）保持空间一致性；即，同一子图像的像素点具有相同的运动  
     *算法步骤：  
      （1）对一个连续的视频帧序列进行处理；  
      （2）针对每一个视频序列，利用一定的目标检测方法，检测可能出现的前景目标；  
      （3）如果某一帧出现了前景目标，找到其具有代表性的关键特征点（可以随机产生，也可以利用角点来做特征点）；  
      （4）对之后的任意两个相邻视频帧而言，寻找上一帧中出现的关键特征点在当前帧中的最佳位置，从而得到前景目标在当前帧中的位置坐标；  
      （5）如此迭代进行，便可实现目标的跟踪；  
     *缺点：  
      （1）不适用于运动物体速度过快的场景  
      （2）不适用于视频亮度变化明显的场景。  
      （3）计算量较大，无法保证实时性和实用性  

   》卡曼尔滤波法：（Kalman Filter)  
   
     	*一种基于线性动态系统的递归估计器。适用于预测物体的未来位置并更新估计。  
     	*卡尔曼滤波器的目标是根据不完美的测量数据，实时地预测和估计某个动态系统的当前状态，例如物体的位置和速度。  
      *算法步骤  
       （1）状态：系统的状态是我们要估计的变量，例如汽车的速度和位置。  
       （2）测量：从传感器或其他数据源获取的观测数据，这些数据通常包含噪声和不准确性。  
       （3）预测：根据当前状态和已知的运动模型，预测下一时刻的状态。  
       （4）校正：根据测量数据，调整预测的状态以获得更精确的估计   
       
   》 均值漂移（Mean Shift）  
   
      *目标：通过计算目标区域的颜色直方图密度，来确定目标在后续帧中的位置  
      *算法步骤：  
      （1）初始化：在视频的第一帧中，手动或通过其他检测方法选择目标区域，并计算该区域的颜色直方图作为目标模型。  
      （2）目标模型计算：使用颜色直方图或其他特征描述符来表示目标区域。常见的选择是RGB或HSV颜色空间。  
      （3）窗口定义：在目标区域中心定义一个搜索窗口，该窗口将用于在后续帧中寻找目标。  
      （4）计算密度梯度：在当前帧中，使用目标模型在搜索窗口内计算密度梯度。具体来说，计算窗口内每个像素点的密度梯度，并求和得到新的中心点。  
      （5）均值漂移迭代：移动窗口的中心到新的中心点。重复计算密度梯度和移动窗口的步骤，直到窗口的中心点不再变化（即收敛）。  
      （6）处理下一帧：在视频的下一帧重复上述步骤，直至所有帧处理完毕。  
      *优点  
      （1）无需训练：均值漂移是基于非参数的方法，不需要预先训练模型。  
      （2）实时性强：算法简单高效，适合实时目标跟踪。  
      （3）适应性强：能够处理目标的部分遮挡和形变。  
      *缺点  
      （1）对初始化敏感：初始目标区域的选择对跟踪效果影响较大。  
      （2）无法处理快速移动目标：对于快速移动或变化较大的目标，均值漂移算法可能跟踪效果不佳。  
      （3）固定窗口大小：搜索窗口的大小是固定的，无法自适应目标的缩放变化。  

  》粒子滤波（Particle Filter）  
  
     *概括：是一种基于贝叶斯理论的递归估计方法，常用于非线性和非高斯的动态系统。它通过大量随机采样（粒子）来表示概率分布，从而实现对目标状态的估计  
     *算法步骤：  
      （1）初始化：在初始时刻生成大量粒子，并根据先验信息初始化它们的状态和权重。  
      （2）预测：根据运动模型，对每个粒子的位置进行预测。运动模型可以是简单的匀速模型、匀加速模型或其他更复杂的模型。  
      （3）更新：根据观测模型，计算每个粒子的权重。权重反映了粒子预测状态与实际观测之间的匹配程度。  
      （4）重采样：根据粒子的权重进行重采样，生成新的粒子集。重采样的目的是保留高权重的粒子，丢弃低权重的粒子，从而聚焦于更可能的目标状态。  
      （5）估计：根据重采样后的粒子集，计算目标状态的估计值。通常是粒子状态的加权平均值。  
     *优点  
      （1）处理非线性和非高斯问题：粒子滤波不需要线性高斯假设，适用于更广泛的问题。  
      （2）灵活性强：可以使用复杂的运动模型和观测模型。  
      （3）鲁棒性高：对目标的形变、遮挡和背景干扰有较强的适应能力。  
     *缺点  
      （1）计算复杂度高：粒子数量较多时，计算量较大，可能导致实时性差。  
      （2）权重退化问题：在迭代过程中，可能大多数粒子的权重会趋近于零，影响估计效果。  
      （3）依赖初始分布：初始粒子的分布对滤波效果影响较大，初始化不当可能导致跟踪失败。  

  
  +基于AI的追踪算法  
  》多目标跟踪算法：  
  （1）StrongSort  
  （2）SMILEtrack  
  （3）Bytetrack
  
 ![bytetrack流程图2](https://github.com/icode-pku-edgeai/tracking/assets/151052189/54a0a4a2-293c-4ebf-9f18-a12b0c9fdd92)

      1. 检测阶段  
      使用高性能检测器（如YOLOX）获取视频中的检测框。这些检测框包含高分和低分的目标检测框。  
      
      2. 初始匹配  
      使用Kalman滤波器预测上一个跟踪轨迹的位置。计算这些预测位置与当前帧中的高分检测框之间的相似度，相似度可以通过IoU（Intersection over Union）或Re-ID（再识别）特征距离来计算。将高分检测框与预测位置进行匹配，形成初步的跟踪轨迹。
      
      3. 二次匹配
      对于未匹配的跟踪轨迹和低分检测框，再次计算它们之间的相似度，并进行匹配。这一步骤主要是为了恢复被遮挡或低分的目标对象，同时过滤掉背景检测框。
      
      4. 轨迹更新
      更新每个匹配后的轨迹，更新其位置、速度等参数。如果某个轨迹未匹配到任何检测框，则该轨迹被标记为丢失轨迹，但仍保留在跟踪列表中，以便在后续帧中可能重新找到。
      
      5. 输出阶段
      输出更新后的轨迹，包括目标的边界框、身份ID等信息。  
      
  （4）sort：   
  
 ![sort流程图](https://github.com/icode-pku-edgeai/tracking/assets/151052189/dc3b69b2-74ac-46f6-ab2e-b8c4acac874a)
  
      1）将第一帧检测到的结果创建其对应的Tracks。将卡尔曼滤波的运动变量初始化，通过卡尔曼滤波预测其对应的框框。

    （2）将该帧目标检测的框框和上一帧通过Tracks预测的框框一一进行IOU匹配，再通过IOU匹配的结果计算其代价矩阵（cost matrix，其计算方式是1-IOU）。
    
    （3）将（2）中得到的所有的代价矩阵作为匈牙利算法的输入，得到线性的匹配的结果，这时候我们得到的结果有三种，第一种是Tracks失配（Unmatched Tracks），我们直接将失配的 
         Tracks删除；第二种是Detections失配（Unmatched Detections），我们将这样的Detections初始化为一个新的Tracks（new Tracks）；第三种是检测框和预测的框框配对成功， 
         这说明我们前一帧和后一帧追踪成功，将其对应的Detections通过卡尔曼滤波更新其对应的Tracks变量。
    
    （4）反复循环（2）-（3）步骤，直到视频帧结束。

    优点：算法速度快，在没有遮挡的情况下，准确度很高。  
    缺点：对物体遮挡的情况几乎没有考虑，所以会导致id switch很频繁，准确率也不高。  


  （5）deepsort：

 ![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/a35839ed-57f9-4990-a40d-79f5f2c7af63)

    （1）将第一帧次检测到的结果创建其对应的Tracks。将卡尔曼滤波的运动变量初始化，通过卡尔曼滤波预测其对应的框框。这时候的Tracks一定是unconfirmed的。
    
    （2）将该帧目标检测的框框和第上一帧通过Tracks预测的框框一一进行IOU匹配，再通过IOU匹配的结果计算其代价矩阵（cost matrix，其计算方式是1-IOU）。
    
    （3）将（2）中得到的所有的代价矩阵作为匈牙利算法的输入，得到线性的匹配的结果，这时候我们得到的结果有三种，第一种是Tracks失配（Unmatched Tracks），我们直接将失配的 
         Tracks（因为这个Tracks是不确定态了，如果是确定态的话则要连续达到一定的次数（默认30次）才可以删除）删除；第二种是Detections失配（Unmatched Detections），我们 
         将这样的Detections初始化为一个新的Tracks（new Tracks）；第三种是检测框和预测的框框配对成功，这说明我们前一帧和后一帧追踪成功，将其对应的Detections通过卡尔曼 
         滤波更新其对应的Tracks变量。
    
    （4）反复循环（2）-（3）步骤，直到出现确认态（confirmed）的Tracks或者视频帧结束。
    
    （5）通过卡尔曼滤波预测其确认态的Tracks和不确认态的Tracks对应的框框。将确认态的Tracks的框框和是Detections进行级联匹配（之前每次只要Tracks匹配上都会保存Detections 
         其的外观特征和运动信息，默认保存前100帧，利用外观特征和运动信息和Detections进行级联匹配,这么做是因为确认态（confirmed）的Tracks和Detections匹配的可能性更大).
    
    （6）进行级联匹配后有三种可能的结果。第一种，Tracks匹配，这样的Tracks通过卡尔曼滤波更新其对应的Tracks变量。第二第三种是Detections和Tracks失配，这时将之前的不确认状态的Tracks和失配的Tracks一起和Unmatched Detections一一进行IOU匹配，再通过IOU匹配的结果计算其代价矩阵（cost matrix，其计算方式是1-IOU）。
    
    （7）将（6）中得到的所有的代价矩阵作为匈牙利算法的输入，得到线性的匹配的结果，这时候我们得到的结果有三种，第一种是Tracks失配（Unmatched Tracks），我们直接将失配的Tracks（因为这个Tracks是不确定态了，如果是确定态的话则要连续达到一定的次数（默认30次）才可以删除）删除；第二种是Detections失配（Unmatched Detections），我们将这样的Detections初始化为一个新的Tracks（new Tracks）；第三种是检测框和预测的框框配对成功，这说明我们前一帧和后一帧追踪成功，将其对应的Detections通过卡尔曼滤波更新其对应的Tracks变量。
    
    （8）反复循环（5）-（7）步骤，直到视频帧结束。


#匈牙利算法： 匹配卡曼尔滤波得到的预测框，和目标检测模型（如yolo，faster-RCNN）得到的检测框， 外观特征用余弦距离（相似程度），运动信息用马氏距离。代价矩阵为这两个距离的加权和。  
#联级匹配 ：对每个跟踪器赋予年龄age（到上一次匹配成功的距离），age越小，越先和检测框集合进行匈牙利匹配。匹配的代价值还是余弦距离和马氏距离的加权和，不过这里当马氏距离大于一定阈值时会让其权重无限大，相当于初筛掉一些检测框。  

在卡尔曼滤波结果中，对于确定态的跟踪框集合我们记为T，当前目标检测出来的结果集合我们记为D。  
1.我们先将这两者T和D内的结果对应的图像送到REID网络中（我们这里先不管得到的是什么），然后去计算网络计算出来结果的余弦距离，以此作为代价矩阵costmatrix这里记为C  
![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/b6bdfb67-de38-49f2-8a20-6d8ab1e17d46)

2.再去计算检测和预测结果的马氏距离，然后根据统计学和实验所得的阈值，去除掉不符合马氏距离和余弦距离阈值的匹配项。（这里马氏距离仅仅作为去除不符合的匹配项而已）  
![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/a75551a8-032a-4f5a-a8cf-e473b53cec51)

3.根据预测框的更新状态（这里更新状态是指，这个预测框距离上一次被匹配成功的时间），越新的预测框（也即距离上一次被匹配上的帧数越短的），越优先根据C的结果进行匈牙利算法进行匹配——也即作者认为，越新的框越有可能和当前的检测框匹配上。

最后根据B中的结果去划分匹配上的集合和未匹配上的集合

![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/b46abea6-263c-4a35-ad1e-a9ab1d69d172)

（注：图中检测框应该改成预测框，age箭头方向应为向下）  

评价指标：  

    •多目标跟踪准确性（MOTA）：从假阳性、假阴性和身份切换的角度总结总体跟踪准确性。
    
    •多目标跟踪精度（MOTP）：根据地面实况和报告位置之间的边界框重叠，总结总体跟踪精度。
    
    •大部分跟踪（MT）：在其寿命的至少80%内具有相同标签的地面实况跟踪的百分比。
    
    •大部分丢失（ML）：在其寿命的20%以下被跟踪的地面实况轨迹的百分比。
    
    •id变换（ID）：地面实况轨道的报告身份发生变化的次数。
    
    •碎片化（FM）：一个视频因缺失检测而中断的次数。




性能比较：  
![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/d4d1663e-4568-4093-aa07-f69217c9600f)

    
  》单目标跟踪算法：  
  （1）ARtrackV2：  
      *论文：ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe   (CVPR 2024)  
      *项目地址：https://github.com/MIV-XJTU/ARTrack    
    
  （2）Dropmae：  
      *论文：DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks  (CVPR 2023)  
      *项目地址：https://github.com/jimmy-dq/DropMAE  
  （3）Siamese系列网络：  
     <1>SiamFC  
     
     ![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/351cdccc-f5e1-49d0-970c-86ad1a0bfc5a)

      SiamFC有两个分支对应两个输入为z（模板图像）和x（搜索区域），将他们同时输入进行特征提取，分别生成6×6×128和22×22×128的feature map。  
      用于特征提取的𝜑代表backbone，用的是AlexNet，并且在SiamFC中，有个很大的特点：即没有在AlexNet中引入padding，这点很重要，因为这保证了网络的全卷积性，也   
      即平移不变性，保证了计算的准确性。  
   ![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/dbba9eee-4420-4458-86bb-55d883f615e1)

      其中，*是卷积操作，即，将模板图像的特征和搜索区域的特征进行卷积，从而获得17×17大小的score map，其中的每一个点都代表搜索区域中该位置是否存在目标的概率。b1
      是bias，在score map的每个位置都加上它。  
      网络生成的是17×17 的heatmap，而输入x是255×255的搜索图像，怎样将两者的位置进行映射呢。这里，作者将17×17的heatmap进行双三次插值生成272×272的图像，来确定物体 
      的 位置。但是为什么不生成255×255的图像呢，作者在论文中说，这样做是由于原始图像相对比较粗糙，为了使定位更加准确
      
    AlexNet:  
   ![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/e6941b0b-03fc-46d7-8c7d-4404a77ac646)

    <2>SiamRPN  :

    SiamRPN中的Siamese network模块与siamFC相同，使用预训练的AlexNet网络，可将其作为一种变换𝜑，将这种变换分别应用到模板支和检测支上，产生模板和搜索区域的特征𝜑(𝑧),𝜑(𝑥)。RPN网络由两部分组成，一部分是分类分支，用于区分目标和背景，另一部分是回归分支，它将候选区域进行微调

  ![SiamPRN算法流程图](https://github.com/icode-pku-edgeai/tracking/assets/151052189/81f824b4-0d16-4513-94b0-af18e6075520)

  ![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/7a4d608a-7dbb-4731-9799-becd229dc16c)


<3>SiamRPN++:
![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/57aaea1d-418c-48cd-bff1-c3dae5b4ea2d)  
虚线的两边都是网络结构图，虚线左侧是特征提取网络结构，右侧是RPN结构图  
三个RPN模块的输出尺寸具有相同的空间分辨率，因此直接对RPN输出采用加权和。  

(4)OStrack:  
网络架构图：
![image](https://github.com/icode-pku-edgeai/tracking/assets/151052189/54363386-4fd5-48f0-a991-69a530158d01)  

推理流程：  

      1.Search_pic和Template_pic进行patch embeding让图片变成一个一个块  
      2.flatten得到template_token(也可称为template_feature) search_token(search_feature) 并加上位置编码，再将两个token拼接起来，  
      送入后面的N个encoder层（N一般取12） 。每第3、6、9（可手动更改，这里给出的是基线模型的设置）个encoder,执行候选消除模块 。  
      3对search_token中消除的块进行padding 得到最终的search_token (由于template_token不会消除模块，所以template_token不变)。 将两个token 再拼接，得到最终的X。  
      4.对X后处理。    

候选消除模块：   

      1.search图像中的每个patch都可以被看作是一个目标候选区域。  
      2.在每个候选消除模块中，每个候选区域会被计算一个与template图像的相似度作为其得分，得分最高的k个候选区域会被保留下来，其他的候选区域则会被丢弃。  
      3.为了避免template中背景区域的影响，在本文中作者并没有使用候选区域与每个template patch计算相似度并取均值，而是直接计算其与template最中心位置的  
        patch之间的相似度作为其得分。可以这样做的原因在于经过self-attention操作之后，中心的template patch已经聚集了足够的目标信息。  
      4.由于这些背景区域不会对分类和回归操作造成影响，因此在Transformer输出之前，这些中途被丢弃的区域直接做zero-padding （作者想法）即可将特征恢复成原尺寸。  




（5）单目标追踪的常用数据集以及性能评价指标：  

    1.GOT-10K是一个通用的目标跟踪数据集，包含了10000个视频序列，覆盖了多种目标类别和场景，主要的评价指标有AO、SR    
        <1>AO：平均重叠率，表示所有基本事实和估计边界框之间重叠的平均值；   
        <2>SR：成功率，测量的是重叠超过阈值（例如，0.5）的成功跟踪帧的百分比。    

    2.VOT是一个针对视觉目标跟踪的挑战性数据集，包含了多个视频序列，并提供了丰富的挑战性标注，如目标尺度变化、遮挡、快速运动等，主要的评价指标有A、R、EAO、EFO。

        <1>准确率Accuracy：在单个测试序列下的平均重叠率（即AOR），只考虑跟踪成功的帧；

        <2>鲁棒性Robustness：在单个测试序列下的跟踪失败的次数，跟踪失败也就是重叠率为0；

        <3>EAO(Expected Average Overlap)：平均重叠期望，数值越大，准确率越高；

        <4>EFO(Equivalent Filter Operations)：用来衡量速度的指标。  

    3.LaSOT是一个大规模的长期跟踪基准，有280个长期测试视频，平均长度为2448帧，主要的评价指标有AUC、NP。

      <1>  AUC：指的是ROC曲线下的面积，使用AUC作为评价指标是因为在某些情况ROC曲线并不能清晰的说明哪个分类器好，而AUC是一个数值，值越大，分类器效果越好；

      <2>NP(Negative Pre)：是指“不匹配”评价指标，用来衡量目标跟踪器在跟踪过程中预测为负类（即未匹配到目标）的准确性。

  4.TrackingNet是指包含511个短期测试视频的大规模视觉跟踪基准，使用的评价指标有：AUC、NP。



2.与检测算法的关系  
  +与YOLO紧密关联的检测算法  
    ARtrack可以很好的利用yolo对追踪目标检测的坐标信息进行进一步追踪。  
3.数据集  
    got10k，  
4.训练  

5.整个推理流程  
  ①前处理  
  ②推理（基于pytorch，onnxruntime，trt）  
  ③后处理  
  ④精度评价  

6.基于python的应用  
  ①pyqt  
  ②gradio


  
  

